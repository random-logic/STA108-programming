```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# 3.2
```{r 3.2}
# Case 1: Error variance decreases with X
# Generate synthetic data ?? wrong
set.seed(123)
X1 <- seq(1, 10, length.out = 100)
Y1 <- 2*X1 + rnorm(100, mean = 0, sd = X1^0.5) # Error variance decreases with X

# Fit linear regression model
model1 <- lm(Y1 ~ X1)

# Residuals for Case 1
residuals1 <- resid(model1)

# Plot residual vs. fitted values
plot(fitted(model1), residuals1, xlab = "Fitted values", ylab = "Residuals",
     main = "Residual Plot: Error Variance Decreases with X")

# Add a horizontal line at y = 0 for reference
abline(h = 0, col = "red")

# Case 2: True regression function is U shaped
# but a linear regression function is fitted
# Generate synthetic data
X2 <- seq(-5, 5, length.out = 100)
# True regression function is U shaped
Y2 <- 2*X2^2 + rnorm(100, mean = 0, sd = 3)

# Fit linear regression model
model2 <- lm(Y2 ~ X2)

# Residuals for Case 2
residuals2 <- resid(model2)

# Plot residual vs. fitted values
plot(fitted(model2), residuals2, xlab = "Fitted values", ylab = "Residuals",
     main = "Residual Plot: True Regression Function is U shaped")

# Add a horizontal line at y = 0 for reference
abline(h = 0, col = "red")
```

# 3.9
```{r 3.9}
# Provided data
x_i <- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11)
e_i <- c(3.2, 2.9, -1.7, -2.0, -2.3, -1.2, -0.9, 0.8, 0.7, 0.5)

# Fit linear regression model
model <- lm(e_i ~ x_i)

# Get residuals
residuals <- resid(model)

# Plot residuals against Xi
plot(x_i, residuals, xlab = "Number of rooms in the home (Xi)",
     ylab = "Residuals (ei)",
     main = "Residual Plot: Household Electricity Consumption")

# Add a horizontal line at y = 0 for reference
abline(h = 0, col = "red")
```

Based on the plot, there does not seem to be a linear relationship between the datapoints. A transformation might alleviate the problem, as it can linearize non-linear relationships.

# 3.18
```{r 3.18}
# Read the txt file into a dataframe
production_data <- read.table("CH03PR18.txt")
colnames(production_data) <- c("Y", "X")
```

a. A linear relationship does appear adequate because it looks like there is a positive linear correlation. We would like to do a transformation on X to not mess up the positive linear correlation. ??
```{r 3.18-a}
# Do plot
plot(production_data$X, production_data$Y,
     xlab = "Production Lot Size",
     ylab = "Production Time (hours)",
     main = "Scatter Plot of Production Data")
```

```{r 3.18-b}
# Transform the data
transformed_X <- sqrt(production_data$X)

# Perform linear regression on the transformed data
lm_result <- lm(production_data$Y ~ transformed_X)

# Extract estimated coefficients
intercept <- coef(lm_result)[1]
coef_sqrtX <- coef(lm_result)[2]
```

b. $\hat{Y_i} = `r round(intercept, 4)` + `r round(coef_sqrtX, 4)` x_i'$

c. The regression line appears to be a good fit of the transformed data.
```{r 3.18-c}
# Plot the transformed data
plot(sqrt(production_data$X), production_data$Y, 
     xlab = "Transformed Production Lot Size (sqrt(X))", 
     ylab = "Production Time (hours)",
     main = "Scatter Plot of Transformed Production Data")

# Add the estimated regression line
abline(lm_result, col = "red")
```

d. My plots show that there is constant variance (because the dots in the residuals plot are spread out approximately equally for different fitted values) and the distribution is normal (because the dots in the normality plot conform to the line).
```{r 3.18-d}
# Obtain the residuals
residuals <- resid(lm_result)

# Plot residuals against fitted values
plot(fitted(lm_result), residuals,
     xlab = "Fitted values", ylab = "Residuals",
     main = "Residuals vs Fitted Values")

# Add a horizontal line at y = 0
abline(h = 0, col = "red")

# Prepare a normal probability plot
qqnorm(residuals, main = "Normal Probability Plot of Residuals")
qqline(residuals)
```

e. $\hat{Y_i} = `r round(intercept, 4)` + `r round(coef_sqrtX, 4)` \sqrt{x_i}$

# 3.19
This difference can arise because ??

# 3.20
The transformation $X' = 1 / X$ maintains a linear regression relationship, so the error terms are still independent and $\epsilon_{ij} \sim N(0, \sigma^2)$. However, the same does not hold after the transformation $Y' = 1 / Y$, as this transformation modifies the error values.

# 3.23
The full model is $Y_{ij} = \mu_j + \epsilon_{ij}$

$df_F = n - c = 20 - 10 = 10$

The reduced model is $Y_{ij} = \beta_1 x_j + \epsilon_{ij}$

$df_R = n - 1 = 20 - 1 = 19$

# 3.24
```{r 3.24}
x <- c(5, 8, 11, 7, 13, 12, 12, 6)
y <- c(63, 67, 74, 64, 75, 69, 90, 60)
```

# Appendix
```{r appendix, results="asis"}
# Get all the labels
all_labels <- knitr::all_labels()

# Print out code along with labels
for (label in all_labels) {
  if (label == "setup" || label == "appendix") {
    next
  }

  code <- knitr::knit_code$get(label)

  cat("##", label, "\n")
  cat("```r \n")
  cat(code, sep = "\n")
  cat("```\n")
}
```